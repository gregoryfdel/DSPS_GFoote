{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gregoryfdel/DSPS_GFoote/blob/main/HW3/KS_earthquakes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk3iBT7phXry"
   },
   "source": [
    "# The Kolmogorovâ€“Smirnov Test and Earthquakes\n",
    "\n",
    "Initally Created by FedericaBBianco @fedhere for DSPS/MLNPS\n",
    "\n",
    "Heavily rewritten and completed by Gregory Foote @gregoryfdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:40:01.895798Z",
     "start_time": "2019-09-09T01:40:01.890474Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LS78mdVthXrz",
    "outputId": "92c6507b-0221-4383-87ae-ebff8c8cbde0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "\n",
    "# Check if using google colab https://stackoverflow.com/questions/53581278/test-if-notebook-is-running-on-google-colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# Use LaTeX and HTML for outputting text and tables\n",
    "from IPython.display import Latex, HTML, display\n",
    "\n",
    "def latex_print(in_string):\n",
    "    \"\"\"\n",
    "    Outputs a string as if it is LaTeX\n",
    "\n",
    "    :param string: Input Python String\n",
    "    :return handle: Output handler for the display object \n",
    "    \"\"\"\n",
    "\n",
    "    in_string = f\"\\\\textnormal{{{in_string}}}\" if IN_COLAB else in_string\n",
    "    return display(Latex(in_string))\n",
    "\n",
    "\n",
    "def html_table(input_list, table_style=\"\" , cell_decorator=None):\n",
    "    \"\"\"\n",
    "    Outputs an 2D iterable as an html table\n",
    "\n",
    "    :param input_list: Python Iterable\n",
    "    :param table_style: CSS which styles the final table\n",
    "    :param cell_decorator:  Callable which will accept the \n",
    "                            cell and it's location and perform\n",
    "                            some action which will alter the \n",
    "                            look of the cell\n",
    "    :return handle: Output handler for the display object\n",
    "    \"\"\"\n",
    "\n",
    "    input_list = np.array(input_list)\n",
    "\n",
    "    output_html = f\"<table style=\\\"{table_style}\\\">\"\n",
    "    for cell_row, row in enumerate(input_list):\n",
    "        output_html += r\"<tr>\"\n",
    "        for cell_col, cell in enumerate(row):\n",
    "            cell_str = str(cell)\n",
    "            if cell_decorator is not None:\n",
    "                cell_str = cell_decorator(cell, cell_row, cell_col)\n",
    "            output_html += f\"<td>{cell_str}</td>\"\n",
    "        output_html += r\"</tr>\"\n",
    "\n",
    "    output_html += r\"</table>\"\n",
    "    return display(HTML(output_html))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run if not in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvIJHRyuhXr2"
   },
   "source": [
    "This homework asks you to reproduce the work in [Corral 2018](https://arxiv.org/pdf/0910.0055.pdf) which is well described, but not \"reproducible\". \n",
    "Corral 2018 uses a K-S test to show that at different magnitude scales the time gaps between earthquakes follows the same distribution. If true, this indicates that there is a consistent scaling law. \n",
    "\n",
    "The argument is a little convoluted, but it is a somewhat innovative use of the test. Corall compares the time gap between earthquakes  greater than a certain magnitude threshold with the time gaps between earthquakes above a different threshold, and finds no differences.\n",
    "\n",
    "Remind yourself exactly what the K-S test is about :\n",
    "\n",
    "    1 What is the test's Null Hypothsis that the K-S test tests?\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8DJjYrthXr3"
   },
   "source": [
    "The K-S test's null hypothesis is that a pair of samples are drawn from the same distribution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8T9XH7nhXr3"
   },
   "source": [
    "    2 What is the \"statistic\" or \"pivotal quantity\" that the test uses?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T21:14:09.757045Z",
     "start_time": "2019-09-08T21:14:09.753882Z"
    },
    "id": "jSSktfhAhXr4"
   },
   "source": [
    "The test statistic for the K-S test is the maximum difference between the two normalized cumulative sample distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T21:14:43.709157Z",
     "start_time": "2019-09-08T21:14:43.704734Z"
    },
    "id": "IYoQgG1BhXr4"
   },
   "source": [
    "    3 What does the probability distribution of this statistic depend on? \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak74uTIrhXr5"
   },
   "source": [
    "The probability distribution of this statistic is only dependent on the amount of data in the two samples.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-h5uhgfhXr6"
   },
   "source": [
    "# Data Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TW_8XaNhXr6"
   },
   "source": [
    "The first reason why the paper is not techincally _reproducible_ is that, while a link is provided to retrieve the data, the link is dead. This happens often. Services like [Zenodo](https://zenodo.org/) or journals that serve data provide some insurance against this but unfortunately the standards are not strict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt6gDmQJhXr7"
   },
   "source": [
    "We can retrieve the earthquake data from [this website](http://service.scedc.caltech.edu/eq-catalogs/poly.php) by making the appropiate POST request, by utilizing the `requests` library in python. By using python to directly query the server, as opposed to using the webform, I can ensure that anyone who looks at my code will be able to download the exact same dataset. Another reason is that others can scrutinize the input data to check for any mistakes I might of made. Because the authors did not comply with reproducibility standards I can only guess to the reigon they chose and ensure the number of entries in is similar to that of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redownloading the data\n",
    "\n",
    "In order to download the data, we can make a POST request to the server with a payload containing all request data. To accomplish this in python, we first need the `requests` library; then we will stream the data output into a file and montior it's progress using `tqdm`. We can also make a GET request to get the already cached data from my github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install requests\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "request_data = {\n",
    "\t\"outputfmt\": \"scec\",\n",
    "\t\"start_year\": \"1984\",\n",
    "\t\"start_month\": \"01\",\n",
    "\t\"start_day\": \"01\",\n",
    "\t\"start_hr\": \"00\",\n",
    "\t\"start_min\": \"00\",\n",
    "\t\"start_sec\": \"00\",\n",
    "\t\"end_year\": \"2002\",\n",
    "\t\"end_month\": \"12\",\n",
    "\t\"end_day\": \"31\",\n",
    "\t\"end_hr\": \"00\",\n",
    "\t\"end_min\": \"00\",\n",
    "\t\"end_sec\": \"00\",\n",
    "\t\"min_mag\": \"2.0\",\n",
    "\t\"max_mag\": \"9.0\",\n",
    "\t\"min_depth\": \"-5.0\",\n",
    "\t\"max_depth\": \"30.0\",\n",
    "\t\"latd1\": \"32.0\",\n",
    "\t\"lond1\": \"-122.0\",\n",
    "\t\"latd2\": \"37.0\",\n",
    "\t\"lond2\": \"-122.0\",\n",
    "\t\"latd3\": \"37.0\",\n",
    "\t\"lond3\": \"-114.0\",\n",
    "\t\"latd4\": \"32.0\",\n",
    "\t\"lond4\": \"-114.0\",\n",
    "\t\"polygoncoords\": \"32.72329178103315,-114.70275878906251;32.72329178103315,-114.70275878906251;35.02234920950592,-114.62036132812501;35.02234920950592,-114.62036132812501;39.01320836803336,-120.0146484375;39.01320836803336,-120.0146484375;36.512285105024866,-123.85986328125001;36.512285105024866,-123.85986328125001;32.57598036624046,-119.42138671875;32.57598036624046,-119.42138671875;32.751708525196584,-114.69726562500001;\",\n",
    "\t\"etype\": \"eq\",\n",
    "\t\"gtype\": \"l\",\n",
    "\t\"file_out\": \"Y\"\n",
    "}\n",
    "\n",
    "\n",
    "if not os.path.exists(\"earthquakes_GregoryFoote.txt\"):\n",
    "\ttry:\n",
    "\t\tresponse = requests.get(\"https://raw.githubusercontent.com/gregoryfdel/DSPS_GFoote/main/HW3/earthquakes_GregoryFoote.txt\", stream=True)\n",
    "\t\tresponse.raise_for_status()\n",
    "\texcept requests.exceptions.HTTPError:\n",
    "\t\tprint(\"'earthquakes_GregoryFoote.txt' not found or accessible from github, recreating with POST request to http://service.scedc.caltech.edu\")\n",
    "\t\tresponse = requests.post('http://service.scedc.caltech.edu/cgi-bin/catalog/catalog_search.pl', data=request_data, stream=True)\n",
    "\t\n",
    "\t# https://stackoverflow.com/questions/43743438/using-tqdm-to-add-a-progress-bar-when-downloading-files\n",
    "\ttotal_size = int(response.headers[\"Content-Length\"])\n",
    "\t\n",
    "\twith open(\"earthquakes_GregoryFoote.txt\", \"w\") as handle:\n",
    "\t\twith tqdm(unit='B', unit_divisor=1024, unit_scale=True, miniters=1, desc=\"earthquakes_GregoryFoote.txt\", total=total_size) as pbar:\n",
    "\t\t\tfor block in response.iter_lines(1024, decode_unicode=True):\n",
    "\t\t\t\thandle.write(block + \"\\n\")\n",
    "\t\t\t\tpbar.update(len(block))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the selection\n",
    "\n",
    "To visualize the selection reigon I chose, we will need three libraries:\n",
    "* The `geopandas` library which uses `pandas` as a backend, to manipulate and store the data\n",
    "* The `shapely` library, to create the polygon\n",
    "* The `contextily` library, which will allow me easily add tile maps from the internet to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install geopandas\n",
    "!pip install shapely\n",
    "!pip install contextily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "from shapely.geometry import Polygon\n",
    "import contextily as ctx\n",
    "\n",
    "parsed_polygon = Polygon([tuple(map(float, x.split(',')))[::-1] for x in request_data['polygoncoords'].split(\";\") if len(x) > 0])\n",
    "\n",
    "# EPSG:3857 is used by MapBox which powers the website https://docs.mapbox.com/help/glossary/projection/\n",
    "geodf = geopandas.GeoDataFrame(geometry=[parsed_polygon], crs='EPSG:3857')\n",
    "ax = geodf.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\n",
    "\n",
    "# EPSG:4326 is used by GPS, so provides decent mapping projection over the entire planet https://epsg.io/4326\n",
    "ctx.add_basemap(ax, zoom=8, crs='EPSG:4326', source=ctx.providers.OpenStreetMap.Mapnik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1: A map highlighting the selection reigon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH0X7d9_hXr8"
   },
   "source": [
    "## Read in Data\n",
    "Now that the data is downloaded, let's analyze it with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:40:02.638703Z",
     "start_time": "2019-09-09T01:40:02.454643Z"
    },
    "id": "hOEZafxahXr9"
   },
   "outputs": [],
   "source": [
    "earthquake_data = pd.read_csv(\n",
    "    \"earthquakes_GregoryFoote.txt\",\n",
    "    parse_dates=[[0,1]], infer_datetime_format=True, keep_date_col=True,\n",
    "    delim_whitespace=True, skipinitialspace=True, skip_blank_lines=True,\n",
    "    skiprows=2, skipfooter=2, engine='python')\n",
    "\n",
    "latex_print(f\"The size of my data table is: {' x '.join(map(str, earthquake_data.shape))}; this compares to the professors data which is 70798 x 34\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the data frame created from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:40:02.661139Z",
     "start_time": "2019-09-09T01:40:02.644720Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "9CHqm426hXsC",
    "outputId": "fe2becaa-ef0a-49ee-8c8a-f1edadd424e7"
   },
   "outputs": [],
   "source": [
    "earthquake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYP53BzDhXsG"
   },
   "source": [
    "### Let's make the data frame a bit more human friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:40:02.791776Z",
     "start_time": "2019-09-09T01:40:02.761489Z"
    },
    "id": "WFIPxVG_hXsH"
   },
   "outputs": [],
   "source": [
    "trimmed_earthquake_data = earthquake_data.rename(columns={\"#YYY/MM/DD_HH:mm:SS.ss\":\"datetime\", \"#YYY/MM/DD\":\"date\", \"HH:mm:SS.ss\":\"time\", \"MAG\":\"mag\"})[[\"datetime\", \"date\", \"time\", \"mag\"]]\n",
    "trimmed_earthquake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T21:28:58.082172Z",
     "start_time": "2019-09-08T21:28:58.078926Z"
    },
    "id": "O9qP9mRShXsP"
   },
   "source": [
    "Right now the _time_, _date_, and _datetime_ columns right now are type 'O' which means object, typically a string. We want to convert these columns to datetime objects.\n",
    "\n",
    "To do this conversion we will use the [astype](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html?highlight=astype#pandas.Series.astype) function to convert all three columns at once. Before this though, we need to replace any timestamp with greater than 60 seconds with 59 seconds; which we do with a combination of the [map](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html?highlight=map#pandas.Series.map) and [re.sub](https://docs.python.org/3/library/re.html#re.sub) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:40:12.489690Z",
     "start_time": "2019-09-09T01:40:02.914845Z"
    },
    "id": "eOUTqBDOhXsQ"
   },
   "outputs": [],
   "source": [
    "for fix_col in ['datetime', 'time']:\n",
    "    trimmed_earthquake_data[fix_col] = trimmed_earthquake_data[fix_col].map(lambda x: re.sub(\"60.\\d+\", \"59.00\", x))\n",
    "\n",
    "trimmed_earthquake_data = trimmed_earthquake_data.astype({\"datetime\":\"datetime64\", \"date\":\"datetime64\", \"time\":\"datetime64\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAmaMzQIhXsW"
   },
   "source": [
    "## Select valuable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3kga7VvhXsZ"
   },
   "source": [
    "Following the description in Section 2  of Corral 2018 I removed all data that did not belong to a \"stationary\" period. Third paragraph section 2 of Corral 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:40:29.230993Z",
     "start_time": "2019-09-09T01:40:29.219163Z"
    },
    "id": "tQRj4CAEhXsZ"
   },
   "outputs": [],
   "source": [
    "goodtimes = [[1984,1986.5],[1990.3, 1992.1],[1994.6, 1995.6],[1996.1, 1996.5], [1997, 1997.6], [1997.75, 1998.15], [1998.25, 1999.35], [2000.55, 2000.8], [2000.9, 2001.25], [2001.6, 2002], [2002.5, 2003]]\n",
    "def get_stationary_index(in_year_frac):\n",
    "    global goodtimes\n",
    "    for year_ind, year_bound in enumerate(goodtimes):\n",
    "            if year_bound[0] <= in_year_frac <= year_bound[1]:\n",
    "                return year_ind\n",
    "    return -1\n",
    "\n",
    "trimmed_earthquake_data['year_frac'] = trimmed_earthquake_data['datetime'].map(lambda x: float(x.year) + float((x - datetime.combine(datetime(x.year, datetime.min.month, datetime.min.day), datetime.min.time())).days/365.25))\n",
    "trimmed_earthquake_data['s_index'] = trimmed_earthquake_data.apply(lambda y: get_stationary_index(y['year_frac']), axis=1)\n",
    "\n",
    "stationary_data = trimmed_earthquake_data[trimmed_earthquake_data['s_index'] > -1]\n",
    "\n",
    "latex_print(f\"There are {len(goodtimes)} timestamp pairs which are the boundaries of good data periods\")\n",
    "latex_print(f\"There are {len(stationary_data)} earthquakes falling in the selected stationary periods\")\n",
    "\n",
    "stationary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:17:07.527314Z",
     "start_time": "2019-09-09T01:17:07.523165Z"
    },
    "id": "P3MKC5tyhXsp"
   },
   "source": [
    "Now what you really want is the _time interval between earthquakes_ for all events greater than some magnitude m, while all you have are the date and time of the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:40:29.292220Z",
     "start_time": "2019-09-09T01:40:29.288359Z"
    },
    "id": "GF-7Xw9AhXsq"
   },
   "outputs": [],
   "source": [
    "large_earthquake_data = stationary_data[stationary_data['mag'] > 1.99]\n",
    "\n",
    "latex_print(f\"There are {len(large_earthquake_data)} earthquakes falling in the selected stationary periods with Magintude 2 and above, this is the same as before because this was selected for in the POST request\")\n",
    "large_earthquake_data['gaps'] = large_earthquake_data['datetime'].diff()\n",
    "latex_print(\"Sample of the Table\")\n",
    "display(large_earthquake_data.head())\n",
    "latex_print(\"Summary of the Numerical Columns in the Table\")\n",
    "display(large_earthquake_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWmOXVyKhO9W"
   },
   "source": [
    "The first entry is NaT is Not a Time, which will interfere with further analysis, so I will remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "kDvKNHD7X1M8",
    "outputId": "e63774d0-9129-4a8c-9a55-918f0deae3c2"
   },
   "outputs": [],
   "source": [
    "gapped_data = large_earthquake_data[1:]\n",
    "gapped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2VH6Q4xhXsw"
   },
   "source": [
    "# data exploration\n",
    "At this point you should wonder if this is the final dataset that you want to use and if anything is weird or suspicious about it. Visualize the distribution. A good way to visualize distributions is a histogram which you can prodice with pl.hist() or as a method of your dataframe series as ```df[SeriesName].plot(kind=\"hist\")```. Produce a plot like the one below (label the axis! and describe it with a caption!). To get the logarithmic y axis you can se ```logy=True```, for example. in your ```df[SeriesName].plot``` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "iDiscxU4XDwg",
    "outputId": "fa5c9392-a6ab-4efe-db17-6ca7b20d35a6"
   },
   "outputs": [],
   "source": [
    "ax = gapped_data['gaps'].map(lambda x: x.total_seconds()).plot(kind=\"hist\", logy=True, title=\"Number of seconds between two consecutive earthquakes\\n during stationary periods\", ylabel=\"Number\")\n",
    "ax.set_xlabel('Time between earthquakes [seconds]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuPhJkrozh91"
   },
   "source": [
    "By limiting our data to the stationary periods, we have introduced an artifical effect from the earthquakes on the boundaries. To remove this effect, we remove any gap that is longer than a month, as that will not happen in natural circumstances as long as the mantle is hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zuGOkWhzhDu"
   },
   "outputs": [],
   "source": [
    "final_data = gapped_data.copy()\n",
    "\n",
    "final_data['gaps_sec'] = final_data.apply(lambda x: x['gaps'].total_seconds(), axis=1)\n",
    "final_data = final_data[final_data['gaps_sec'] < 2592000]\n",
    "\n",
    "ax = final_data['gaps_sec'].plot(kind=\"hist\", logy=True, title=\"Number of seconds between two consecutive earthquakes\\n during stationary periods\", ylabel=\"Number\")\n",
    "ax.set_xlabel('Time between earthquakes [seconds]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gn-HuVevYOo7"
   },
   "source": [
    "## Data Analysis\n",
    "\n",
    "To begin our data analysis, we first choose a p-value of 3-$\\sigma$ as the threshold for rejecting the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt5MAQEZhXsz"
   },
   "source": [
    "Follow the instructions (algorithm) in **Section 3 paragraph 3** and compare your results with the results in table 1 with a threshold  of = 0.01 and 0.001\n",
    "\n",
    "Do it for all 5 magnitude thresholds as indicated in the paper (and in Table 1).\n",
    "\n",
    "Note that the x axis in plot Fig 1 is in _log space_. Use ```np.log10()``` to take the logarithm of the time gaps.\n",
    "\n",
    "The pseudocode for the algorithm is [here](https://github.com/fedhere/DSPS/blob/master/HW3/Corral2018_pseudocode.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXWK4x4FhXs1"
   },
   "source": [
    "Reproduce the paper Fig 1 and Table 1. In the Table report the size of each dataset after cleaning the value of the statistic and the p-value, as done in Corral 2018. Use the scipy function for the 2 sample KS test. (resources [here](https://colab.research.google.com/notebooks/markdown_guide.ipynb#scrollTo=70pYkR9LiOV0) to learn about the table syntax in markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T01:51:40.043756Z",
     "start_time": "2019-09-09T01:51:39.866325Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "BNtqqaXphXs2",
    "outputId": "d7a838f4-9f01-4ebf-ff39-a480a3146e07"
   },
   "outputs": [],
   "source": [
    "mag_limits = [20, 25, 30, 35, 40]\n",
    "data_cache = {}\n",
    "final_tables = []\n",
    "\n",
    "for t_i, threshold in enumerate([0.01, 0.001]):\n",
    "    data_cache[t_i] = {}\n",
    "    for test_mag in mag_limits:\n",
    "        test_data = final_data[final_data['mag'] > float(test_mag/10)]['gaps_sec']\n",
    "        for _ in range(2):\n",
    "            test_data = (test_data / test_data.mean()).loc[lambda x: x > threshold]\n",
    "        data_cache[t_i][test_mag] = test_data\n",
    "    \n",
    "    out_table = np.zeros((len(data_cache[t_i]) + 1, len(data_cache[t_i]) + 2))\n",
    "    for mag_i, data_i in data_cache[t_i].items():\n",
    "        ind_i = mag_limits.index(mag_i)\n",
    "        for mag_j, data_j in data_cache[t_i].items():\n",
    "            ind_j = mag_limits.index(mag_j)\n",
    "            if ind_i == ind_j:\n",
    "                continue\n",
    "            test_rv = ks_2samp(data_i, data_j)\n",
    "            out_table[ind_i + 1, ind_j + 2] = test_rv[(0 if ind_i > ind_j else 1)]\n",
    "    \n",
    "    out_table[0, 0] = threshold\n",
    "    final_tables.append(out_table.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plot_data = [[], []]\n",
    "for k,v in data_cache[0].items():\n",
    "    plot_data[0].append(v)\n",
    "    plot_data[1].append(k)\n",
    "\n",
    "logged_data = [np.log10(np.array(x)) for x in plot_data[0]]\n",
    "ax.hist(logged_data, bins=100, density=True, cumulative=True, histtype=\"step\", label=plot_data[1])\n",
    "ax.set_xlabel(\"x/Rs\", fontsize='xx-large')\n",
    "ax.set_ylabel(\"p(x > x/Rs)\", fontsize='xx-large')\n",
    "ax.legend(labels=[\"M $\\geq \" + str(np.around(x/10,1)) + \"$\" for x in plot_data[1]])\n",
    "x_tick_labels = [str(np.around(10**float(x),3)) for x in ax.get_xticks().tolist()]\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2: The normalized cumlative distribution of the gaps between earthquakes during stationary period. We see visually that the distributions are very simliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_limits = [20, 25, 30, 35, 40]\n",
    "temp_table = None\n",
    "\n",
    "def ordered_tuple_equal(tup_1, tup_2):\n",
    "    return sum(x == tup_2[i] for i, x in enumerate(tup_1)) == len(tup_1)\n",
    "\n",
    "def cell_maker(in_val, in_row, in_col):\n",
    "    in_coord = (in_row, in_col)\n",
    "    table_coord = (in_row - 1, in_col - 2)\n",
    "    if ordered_tuple_equal(in_coord, (0, 0)):\n",
    "        return f\"m = {in_val}\"\n",
    "    elif ordered_tuple_equal(in_coord, (0, 1)):\n",
    "        return \"N\"\n",
    "    elif in_coord[0] == 0:\n",
    "        return f\"M &#8805; {np.around(mag_limits[in_coord[1] - 2]/10., 1)}\"\n",
    "    elif in_coord[1] == 0:\n",
    "        return f\"M &#8805; {np.around(mag_limits[in_coord[0] - 1]/10., 1)}\"\n",
    "    elif in_coord[1] == 1:\n",
    "        return str(len(temp_table[mag_limits[in_coord[0] - 1]]))\n",
    "    elif table_coord[0] == table_coord[1]:\n",
    "        return \"--\"\n",
    "    elif table_coord[0] < table_coord[1]:\n",
    "        return f\"{np.around(in_val * 100, 1)}%\"\n",
    "    else:\n",
    "        return f\"{np.around(in_val, 3)}\"\n",
    "\n",
    "for table_i, out_table in enumerate(final_tables):\n",
    "    temp_table = data_cache[table_i]\n",
    "    html_table(out_table, cell_decorator=cell_maker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y4wb6lEZsGV"
   },
   "source": [
    "Did you find any statistical significant differences between the distributions? What does it mean? Is your result identical to Correll's 2018? Why or why not? **Discuss!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bQwtZi4Z0pi"
   },
   "source": [
    "### \"extra credits\"\n",
    "\n",
    "**How could you _force_ a significant result?**\n",
    "Organize your result for different magnitude threshold in a numpy array (it should be a 5x5 array) for both cutoffs (0.01 and 0.001). Each of these arrays should contain the p-value for the pair of distributions i,j in cell \\[i\\]\\[j\\] and \\[j\\]\\[i\\]. Use ```imshow``` to visualize this 2D data. FIrst visualize the matrix itself as done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T02:02:18.672458Z",
     "start_time": "2019-09-09T02:02:18.553137Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "mqQSdpVEhXs5",
    "outputId": "f6a3e3c6-0706-44b9-995b-73321f492bf4"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "        \n",
    "pl.imshow((ks_001));\n",
    "pl.axis('off')\n",
    "cb = pl.colorbar()\n",
    "cb.ax.set_ylabel(r'$p$-value')\n",
    "pl.title(\"KS test results\");\n",
    "#add a caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSJmSMCmmA3w"
   },
   "source": [
    "\n",
    "Now visualize the result as a matrix where the cells are white if the results is not statistically significant and red otherwise. \n",
    "After doing it fot the set alpha threshold, lower your alpha threshold so that at least one pair of distribution has a statistically significant difference. **Warning!! this is an _unethical and horrifying practice_! Once you chose your significance threshold you are never allowed to change it! Why? Discuss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txEYubwCcIXP"
   },
   "source": [
    "Redoing it for threshold 0001"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "KS_earthquakes.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ec9adc79684924107444e62f99e588cb38f7190c8e84f18e1083d2876bd57627"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
